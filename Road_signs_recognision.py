# -*- coding: utf-8 -*-
"""Kopia notatnika Kopia notatnika Kopia notatnika MSI projekt

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ch1AHiOTRDeYl_gpKsi9Gr9PSG7pqbfa

#Czym jest Colab?

Colab, lub inaczej „Colaboratory”, pozwala Ci pisać i wykonywać kod w języku Python bezpośrednio w przeglądarce

#Importowanie

większość bibliotek jest dosępnych w Colabie od razu, tak jak te przykładowe poniżej. Wystarczy je po prostu zaimportować
"""

import numpy as np
import torch
import zipfile
import matplotlib.pyplot as plt
import json
from PIL import Image
import os
import pandas as pd
from matplotlib.patches import Rectangle
from collections import Counter
import seaborn as sns
import torchvision
import albumentations as A
import cv2
import math
from albumentations.pytorch.transforms import ToTensorV2
import torch.nn as nn
import xml.etree.ElementTree as ET

"""Te których brakuje można zainstalować przy użyciu komendy pip"""

!pip install matplotlib

"""Aby skopiować repozytorium z githuba należy podać komendę git"""

!git clone https://github.com/matplotlib/matplotlib.git

"""Podłączenie dysku google"""

from google.colab import drive
drive.mount('/content/drive')

"""Podłączenie datasetu z kaggle:"""

#stworzenie pliku json
dictionary = {"username":"zespol5","key":"6305f06c79ff7cea5db1fb1e173df581"}
with open("kaggle.json", "w") as outfile:
    json.dump(dictionary, outfile)
!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets list

#pobranie datasetu
!kaggle datasets download -d andrewmvd/road-sign-detection

with zipfile.ZipFile("road-sign-detection.zip",'r') as zip_ref:
  zip_ref.extractall("dataset")

"""#Formularze interaktywne

wyświetlanie obrazów
"""

def get_bboxes(path_to_ann):
    tree = ET.parse(path_to_ann)
    root = tree.getroot()

    annotations = []

    for bndbox in root.iter('bndbox'):
        xmin = int(bndbox.find('xmin').text)
        ymin = int(bndbox.find('ymin').text)
        xmax = int(bndbox.find('xmax').text)
        ymax = int(bndbox.find('ymax').text)

        annotations.append([xmin, ymin, xmax, ymax])
    return annotations

img_dir = "dataset/images"
ann_dir = "dataset/annotations"

img_list = [os.path.join(img_dir, filename) for filename in os.listdir(img_dir)]
ann_list = [os.path.join(ann_dir, filename) for filename in os.listdir(ann_dir)]

img_list.sort()
ann_list.sort()

# Display images with bounding boxes and label based on specified ranges
for i in range(35):
    img_path = img_list[i]
    ann_path = ann_list[i]

    img = np.asarray(Image.open(img_path))
    bboxes = get_bboxes(ann_path)

    plt.imshow(img)
    ax = plt.gca()

    for bbox in bboxes:
        x = bbox[0]
        y = bbox[1]
        width = bbox[2] - bbox[0]
        height = bbox[3] - bbox[1]
        rect = Rectangle((x, y), width, height, linewidth=1, edgecolor='r', facecolor='none')
        ax.add_patch(rect)

    # Labeling based on specified ranges
    if i+1 in range(28, 36):
        plt.title("Informacyjne")  # Information Signs
    elif 1 <= i+1 <= 3 or i+1 == 14 or i+1 == 25:
        plt.title("Sygnalizacja świetlna")  # Traffic Signals
    elif 4 <= i+1 <= 27:
        plt.title("Ograniczenia predkosci")  # Speed Limits

    plt.axis('off')
    plt.show()

"""Statystyki"""

def get_bboxes(path_to_ann):
    tree = ET.parse(path_to_ann)
    root = tree.getroot()

    annotations = []

    for bndbox in root.iter('bndbox'):
        xmin = int(bndbox.find('xmin').text)
        ymin = int(bndbox.find('ymin').text)
        xmax = int(bndbox.find('xmax').text)
        ymax = int(bndbox.find('ymax').text)

        annotations.append([xmin, ymin, xmax, ymax])

    return annotations

img_dir = "dataset/images"
ann_dir = "dataset/annotations"

img_list = [os.path.join(img_dir, filename) for filename in os.listdir(img_dir)]
ann_list = [os.path.join(ann_dir, filename) for filename in os.listdir(ann_dir)]

img_list.sort()
ann_list.sort()

image_sizes = []
bbox_positions = []
bbox_sizes = []
bbox_counts = []
aspect_ratios = []

for img_path, ann_path in zip(img_list, ann_list):
    img = Image.open(img_path)
    width, height = img.size
    image_sizes.append((width, height))

    bboxes = get_bboxes(ann_path)
    bbox_counts.append(len(bboxes))

    x_positions = [bbox[0] for bbox in bboxes]
    y_positions = [bbox[1] for bbox in bboxes]
    bbox_positions.extend(list(zip(x_positions, y_positions)))

    bbox_widths = [bbox[2] - bbox[0] for bbox in bboxes]
    bbox_heights = [bbox[3] - bbox[1] for bbox in bboxes]
    bbox_sizes.extend(list(zip(bbox_widths, bbox_heights)))

    aspect_ratios.append(height / width)

# Obliczanie średnich wartości
average_image_size = np.mean(image_sizes, axis=0)
average_bbox_position = np.mean(bbox_positions, axis=0)
average_bbox_size = np.mean(bbox_sizes, axis=0)
average_bbox_count = np.mean(bbox_counts)
average_aspect_ratio = np.mean(aspect_ratios)

# Wyświetlanie statystyk liczbowych
print("Statystyki:")
print(f"1. Średni rozmiar obrazu: {average_image_size}")
print(f"2. Średnie umieszczenie bounding boxów (X, Y): {average_bbox_position}")
print(f"3. Średni rozmiar bounding boxów (szerokość, wysokość): {average_bbox_size}")
print(f"4. Średnia ilość bounding boxów na obraz: {average_bbox_count}")
print(f"5. Średni stosunek wysokości do szerokości obrazu: {average_aspect_ratio}")

# Przygotowanie danych do wykresu punktowego
x_positions, y_positions = zip(*bbox_positions)
bbox_widths, bbox_heights = zip(*bbox_sizes)

# Wykresy punktowe
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.scatter(x_positions, y_positions, alpha=0.5)
plt.title('Umieszczenie bounding boxów')
plt.xlabel('X')
plt.ylabel('Y')

plt.subplot(1, 2, 2)
plt.scatter(bbox_widths, bbox_heights, alpha=0.5)
plt.title('Rozmiar bounding boxów')
plt.xlabel('Szerokość')
plt.ylabel('Wysokość')

plt.tight_layout()
plt.show()

"""Podział na klasy"""

# Ścieżki do katalogów obrazów i adnotacji
img_dir = "dataset/images"
ann_dir = "dataset/annotations"

# Lista nazw plików obrazów i adnotacji
img_list = [os.path.join(img_dir, filename) for filename in os.listdir(img_dir)]
ann_list = [os.path.join(ann_dir, filename) for filename in os.listdir(ann_dir)]

# Słownik mapujący nazwy klas na indeksy klas
class_dict = {
    "stop": 0,
    "speedlimit": 1,
    "trafficlight": 2,
    "crosswalk": 3
}

# Przypisz klasy do obrazów na podstawie typów znaków w plikach annotations
image_classes = []
for ann_path in ann_list:
    tree = ET.parse(ann_path)
    root = tree.getroot()

    class_indices = set()
    for name_tag in root.iter('name'):
        class_name = name_tag.text
        class_indices.add(class_dict.get(class_name))

    image_name = os.path.splitext(os.path.basename(ann_path))[0].replace('ann', 'img')
    image_classes.append((image_name, list(class_indices)))

# Wyświetl mapowanie nazw klas na indeksy klas
print("Mapowanie nazw klas na indeksy klas:")
for class_name, class_index in class_dict.items():
    print(f"{class_name}: {class_index}")

# Wyświetl przykładowe przypisania klas do obrazów
print("\nPrzykładowe przypisania klas do obrazów:")
for image_name, classes in image_classes:
    print(f"Obraz {image_name}: {classes}")

"""Wyświetlanie obrazków po zgrupowaniu"""

def get_bboxes(path_to_ann):
    tree = ET.parse(path_to_ann)
    root = tree.getroot()

    annotations = []

    for bndbox in root.iter('bndbox'):
        xmin = int(bndbox.find('xmin').text)
        ymin = int(bndbox.find('ymin').text)
        xmax = int(bndbox.find('xmax').text)
        ymax = int(bndbox.find('ymax').text)

        annotations.append([xmin, ymin, xmax, ymax])

    return annotations

# Ścieżki do katalogów obrazów i adnotacji
img_dir = "dataset/images"
ann_dir = "dataset/annotations"

# Lista nazw plików obrazów i adnotacji
img_list = [os.path.join(img_dir, filename) for filename in os.listdir(img_dir)]
ann_list = [os.path.join(ann_dir, filename) for filename in os.listdir(ann_dir)]

# Słownik mapujący nazwy klas na indeksy klas
class_dict = {
    "stop": 0,
    "speedlimit": 1,
    "trafficlight": 2,
    "crosswalk": 3
}

# Przypisz klasy do obrazów na podstawie typów znaków w plikach annotations
image_classes = []
for ann_path in ann_list:
    tree = ET.parse(ann_path)
    root = tree.getroot()

    class_indices = set()
    for name_tag in root.iter('name'):
        class_name = name_tag.text
        class_indices.add(class_dict.get(class_name))

    image_name = os.path.splitext(os.path.basename(ann_path))[0].replace('ann', 'img')
    image_classes.append((image_name, list(class_indices)))

# Rysuj po 3 zdjęcia z każdej klasy
for class_index in range(4):
    class_images = [image for image, classes in image_classes if class_index in classes]
    print(f"Klasa: {list(class_dict.keys())[class_index]}")
    for i in range(min(3, len(class_images))):
        img_path = os.path.join(img_dir, f"{class_images[i]}.png")
        ann_path = os.path.join(ann_dir, f"{class_images[i]}.xml")

        img = np.asarray(Image.open(img_path))
        bboxes = get_bboxes(ann_path)

        plt.imshow(img)
        ax = plt.gca()

        for bbox in bboxes:
            x = bbox[0]
            y = bbox[1]
            width = bbox[2] - bbox[0]
            height = bbox[3] - bbox[1]
            rect = plt.Rectangle((x, y), width, height, linewidth=1, edgecolor='r', facecolor='none')
            ax.add_patch(rect)

        plt.title(f"Klasa: {list(class_dict.keys())[class_index]}")
        plt.axis('off')
        plt.show()

"""Statystyki dla znaków w odpowiedniej klasie jako porownanie"""

def get_bboxes(path_to_ann):
    tree = ET.parse(path_to_ann)
    root = tree.getroot()

    annotations = []

    for bndbox in root.iter('bndbox'):
        xmin = int(bndbox.find('xmin').text)
        ymin = int(bndbox.find('ymin').text)
        xmax = int(bndbox.find('xmax').text)
        ymax = int(bndbox.find('ymax').text)

        annotations.append([xmin, ymin, xmax, ymax])


    return annotations

img_dir = "dataset/images"
ann_dir = "dataset/annotations"

img_list = [os.path.join(img_dir, filename) for filename in os.listdir(img_dir)]
ann_list = [os.path.join(ann_dir, filename) for filename in os.listdir(ann_dir)]

# Słownik mapujący nazwy klas na indeksy klas
class_dict = {
    "stop": 0,
    "speedlimit": 1,
    "trafficlight": 2,
    "crosswalk": 3
}

# Przypisz klasy do obrazów na podstawie typów znaków w plikach annotations
image_classes = []
for ann_path in ann_list:
    tree = ET.parse(ann_path)
    root = tree.getroot()

    class_indices = set()
    for name_tag in root.iter('name'):
        class_name = name_tag.text
        class_indices.add(class_dict.get(class_name))

    image_name = os.path.splitext(os.path.basename(ann_path))[0].replace('ann', 'img')
    image_classes.append((image_name, list(class_indices)))

# Statystyki dla każdej klasy
for class_name, class_index in class_dict.items():
    print(f"\nKlasa: {class_name}")
    class_images = [image for image, classes in image_classes if class_index in classes]

    image_sizes = []
    bbox_positions = []
    bbox_sizes = []
    bbox_counts = []
    aspect_ratios = []

    for img_path, ann_path in zip(img_list, ann_list):
        if os.path.splitext(os.path.basename(img_path))[0] in class_images:
            img = Image.open(img_path)
            width, height = img.size
            image_sizes.append((width, height))

            bboxes = get_bboxes(ann_path)
            bbox_counts.append(len(bboxes))

            x_positions = [bbox[0] for bbox in bboxes]
            y_positions = [bbox[1] for bbox in bboxes]
            bbox_positions.extend(list(zip(x_positions, y_positions)))

            bbox_widths = [bbox[2] - bbox[0] for bbox in bboxes]
            bbox_heights = [bbox[3] - bbox[1] for bbox in bboxes]
            bbox_sizes.extend(list(zip(bbox_widths, bbox_heights)))

            aspect_ratios.append(height / width)

    # Obliczanie średnich wartości
    average_image_size = np.mean(image_sizes, axis=0)
    average_bbox_position = np.mean(bbox_positions, axis=0)
    average_bbox_size = np.mean(bbox_sizes, axis=0)
    average_bbox_count = np.mean(bbox_counts)
    average_aspect_ratio = np.mean(aspect_ratios)

    # Liczba obrazów pasujących do klasy
    class_image_count = len(class_images)

    # Wyświetlanie statystyk liczbowych
    print("Statystyki:")
    print(f"1. Średni rozmiar obrazu: {average_image_size}")
    print(f"2. Średnie umieszczenie bounding boxów (X, Y): {average_bbox_position}")
    print(f"3. Średni rozmiar bounding boxów (szerokość, wysokość): {average_bbox_size}")
    print(f"4. Średnia ilość bounding boxów na obraz: {average_bbox_count}")
    print(f"5. Średni stosunek wysokości do szerokości obrazu: {average_aspect_ratio}")
    print(f"Liczba obrazów w klasie: {class_image_count}\n")

    # Przygotowanie danych do wykresu punktowego
    x_positions, y_positions = zip(*bbox_positions)
    bbox_widths, bbox_heights = zip(*bbox_sizes)

    # Wykresy punktowe
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.scatter(x_positions, y_positions, alpha=0.5)
    plt.title('Umieszczenie bounding boxów')
    plt.xlabel('X')
    plt.ylabel('Y')

    plt.subplot(1, 2, 2)
    plt.scatter(bbox_widths, bbox_heights, alpha=0.5)
    plt.title('Rozmiar bounding boxów')
    plt.xlabel('Szerokość')
    plt.ylabel('Wysokość')

    plt.tight_layout()
    plt.show()

"""Udokumentowanie treningu"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, models
from PIL import Image
import xml.etree.ElementTree as ET
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Ścieżki do katalogów obrazów i adnotacji
img_dir = "dataset/images"
ann_dir = "dataset/annotations"

# Lista nazw plików obrazów i adnotacji
img_list = [os.path.join(img_dir, filename) for filename in os.listdir(img_dir)]
ann_list = [os.path.join(ann_dir, filename) for filename in os.listdir(ann_dir)]

# Słownik mapujący nazwy klas na indeksy klas
class_dict = {
    "stop": 0,
    "speedlimit": 1,
    "trafficlight": 2,
    "crosswalk": 3
}

# Tworzenie klasy Dataset
class CustomDataset(Dataset):
    def __init__(self, img_list, ann_list, class_dict, transform=None):
        self.img_list = img_list
        self.ann_list = ann_list
        self.class_dict = class_dict
        self.transform = transform

    def __len__(self):
        return len(self.img_list)

    def __getitem__(self, idx):
        img_path = self.img_list[idx]
        ann_path = self.ann_list[idx]
        image = Image.open(img_path).convert('RGB')

        tree = ET.parse(ann_path)
        root = tree.getroot()
        class_indices = []
        for name_tag in root.iter('name'):
            class_name = name_tag.text
            class_indices.append(self.class_dict.get(class_name))

        # Zmiana listy indeksów klas na pojedynczą etykietę klasy
        class_label = max(class_indices)

        if self.transform:
            image = self.transform(image)

        return image, class_label

# Definiowanie transformacji dla danych treningowych
data_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Tworzenie datasetu i DataLoadera
dataset = CustomDataset(img_list, ann_list, class_dict, transform=data_transform)
train_loader = DataLoader(dataset, batch_size=8, shuffle=True)

# Użycie modelu ResNet18
model = models.resnet18(pretrained=True)
num_ftrs = model.fc.in_features
model.fc = nn.Linear(num_ftrs, len(class_dict))

# Funkcja do przeprowadzania eksperymentów z różnymi parametrami
def run_experiment(train_loader, num_epochs, learning_rate, batch_size, optimizer_type):
    model.train()

    # Wybór optymalizatora
    if optimizer_type == "Adam":
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    elif optimizer_type == "SGD":
        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)
    else:
        raise ValueError("Nieobsługiwany typ optymalizatora.")

    criterion = nn.CrossEntropyLoss()

    # Przeprowadzenie treningu
    loss_values = []
    all_labels = []
    all_preds = []

    for epoch in range(num_epochs):
        running_loss = 0.0
        for i, data in enumerate(train_loader, 0):
            inputs, labels = data
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

            # Zbieranie danych do wizualizacji tylko w ostatniej epoce
            if epoch == num_epochs - 1:
                _, preds = torch.max(outputs, 1)
                all_labels.extend(labels.cpu().numpy())
                all_preds.extend(preds.cpu().numpy())

        epoch_loss = running_loss / len(train_loader)
        loss_values.append(epoch_loss)
        print(f"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}")

    return model, loss_values, all_labels, all_preds

# Parametry eksperymentu
num_epochs = 10
learning_rate = 0.001
batch_size = 64
optimizer_type = "SGD"

# Przeprowadzenie eksperymentu
trained_model, loss_values, all_labels, all_preds = run_experiment(train_loader, num_epochs, learning_rate, batch_size, optimizer_type)

# Wizualizacja strat
plt.figure(figsize=(10, 5))
plt.plot(range(1, num_epochs + 1), loss_values, marker='o')
plt.title('Loss w funkcji epok')
plt.xlabel('Epoki')
plt.ylabel('Loss')
plt.grid(True)
plt.show()

# Obliczanie macierzy pomyłek
cm = confusion_matrix(all_labels, all_preds)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(class_dict.keys()))
disp.plot(cmap=plt.cm.Blues)
plt.title('Macierz pomyłek')
plt.show()

# Wypisanie klas rzeczywistych i przewidzianych
print("Klasy rzeczywiste:")
print(all_labels)
print("Klasy przewidziane przez model:")
print(all_preds)

#zapisanie modelu
torch.save(model.state_dict(),"model_zapisany");
#load

"""Obliczenie metryk"""

# Obliczanie metryk
def compute_metrics(all_labels, all_preds, class_dict):
    # Converting lists to numpy arrays
    all_labels = np.array(all_labels)
    all_preds = np.array(all_preds)

    # Accuracy
    accuracy = np.mean(all_labels == all_preds)

    # Precision and Recall
    precision = precision_score(all_labels, all_preds, average='macro')
    recall = recall_score(all_labels, all_preds, average='macro')

    # Mean Average Precision (mAP)
    # Binarize the output for each class
    y_true_bin = label_binarize(all_labels, classes=list(class_dict.values()))
    y_pred_bin = label_binarize(all_preds, classes=list(class_dict.values()))

    # Compute mAP for each class and take the mean
    ap_scores = [average_precision_score(y_true_bin[:, i], y_pred_bin[:, i]) for i in range(y_true_bin.shape[1])]
    mAP = np.mean(ap_scores)

    return accuracy, precision, recall, mAP

# Compute metrics
accuracy, precision, recall, mAP = compute_metrics(all_labels, all_preds, class_dict)
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"Mean Average Precision (mAP): {mAP:.4f}")

"""Usprawnienie treningu przez augumentacje"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import datasets, models, transforms
from PIL import Image
import xml.etree.ElementTree as ET
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, average_precision_score
import numpy as np
from sklearn.preprocessing import label_binarize
import random

# Function to extract bounding boxes from XML annotation file
def get_bboxes(path_to_ann):
    tree = ET.parse(path_to_ann)
    root = tree.getroot()
    annotations = []
    for bndbox in root.iter('bndbox'):
        xmin = int(bndbox.find('xmin').text)
        ymin = int(bndbox.find('ymin').text)
        xmax = int(bndbox.find('xmax').text)
        ymax = int(bndbox.find('ymax').text)
        annotations.append([xmin, ymin, xmax, ymax])
    return annotations

# Paths to image and annotation directories
img_dir = "dataset/images"
ann_dir = "dataset/annotations"

# Lists of image and annotation filenames
img_list = [os.path.join(img_dir, filename) for filename in os.listdir(img_dir)]
ann_list = [os.path.join(ann_dir, filename) for filename in os.listdir(ann_dir)]

# Dictionary mapping class names to class indices
class_dict = {
    "stop": 0,
    "speedlimit": 1,
    "trafficlight": 2,
    "crosswalk": 3
}

# Custom dataset class
class CustomDataset(Dataset):
    def __init__(self, img_list, ann_list, class_dict, transform=None):
        self.img_list = img_list
        self.ann_list = ann_list
        self.class_dict = class_dict
        self.transform = transform

    def __len__(self):
        return len(self.img_list)

    def __getitem__(self, idx):
        img_path = self.img_list[idx]
        ann_path = self.ann_list[idx]
        image = Image.open(img_path).convert('RGB')

        tree = ET.parse(ann_path)
        root = tree.getroot()
        class_indices = []
        for name_tag in root.iter('name'):
            class_name = name_tag.text
            class_indices.append(self.class_dict.get(class_name))

        # Change list of class indices to a single class label
        class_label = max(class_indices)

        if self.transform:
            image = self.transform(image)

        return image, class_label, img_path, ann_path

# Define data transformations for training and testing
data_transforms = {
    "augment1": transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomHorizontalFlip(),  # Augmentation 1: Random horizontal flip
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ]),
    "augment2": transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  # Augmentation 2: Random color jitter
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ]),
    "augment3": transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomRotation(20),  # Augmentation 3: Random rotation
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ]),
    "test": transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
}

# Create dataset and DataLoader for each augmentation
datasets = {x: CustomDataset(img_list, ann_list, class_dict, transform=data_transforms[x]) for x in data_transforms}
loaders = {x: DataLoader(datasets[x], batch_size=8, shuffle=True) for x in datasets}

# Function to get the ResNet-18 model
def get_resnet18(num_classes):
    model = models.resnet18(pretrained=True)
    num_ftrs = model.fc.in_features
    model.fc = nn.Linear(num_ftrs, num_classes)
    return model

# Function to run an experiment with different parameters
def run_experiment(train_loader, num_epochs, learning_rate, batch_size, optimizer_type):
    # Create the model
    model = get_resnet18(num_classes=len(class_dict))
    model.train()

    # Choose optimizer
    if optimizer_type == "Adam":
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    elif optimizer_type == "SGD":
        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)
    else:
        raise ValueError("Unsupported optimizer type.")

    criterion = nn.CrossEntropyLoss()

    # Training loop
    loss_values = []
    all_labels = []
    all_preds = []
    all_img_paths = []
    all_ann_paths = []

    for epoch in range(num_epochs):
        running_loss = 0.0
        epoch_labels = []
        epoch_preds = []
        for i, data in enumerate(train_loader, 0):
            inputs, labels, img_paths, ann_paths = data
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

            # Collect data for visualization only in the last epoch
            if epoch == num_epochs - 1:
                _, preds = torch.max(outputs, 1)
                epoch_labels.extend(labels.cpu().numpy())
                epoch_preds.extend(preds.cpu().numpy())
                all_img_paths.extend(img_paths)
                all_ann_paths.extend(ann_paths)

        epoch_loss = running_loss / len(train_loader)
        loss_values.append(epoch_loss)
        print(f"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss}")

        if epoch == num_epochs - 1:
            all_labels = epoch_labels
            all_preds = epoch_preds

    return model, loss_values, all_labels, all_preds, all_img_paths, all_ann_paths

# Experiment parameters
num_epochs = 10  # Increase the number of epochs as needed
learning_rate = 0.001  # Use lower learning rate for better convergence
batch_size = 64
optimizer_type = "SGD"  # Use Adam optimizer

# Function to compute metrics
def compute_metrics(all_labels, all_preds, class_dict):
    # Converting lists to numpy arrays
    all_labels = np.array(all_labels)
    all_preds = np.array(all_preds)

    # Accuracy
    accuracy = np.mean(all_labels == all_preds)

    # Precision and Recall
    precision = precision_score(all_labels, all_preds, average='macro')
    recall = recall_score(all_labels, all_preds, average='macro')

    # Mean Average Precision (mAP)
    # Binarize the output for each class
    y_true_bin = label_binarize(all_labels, classes=list(class_dict.values()))
    y_pred_bin = label_binarize(all_preds, classes=list(class_dict.values()))

    # Compute mAP for each class and take the mean
    ap_scores = [average_precision_score(y_true_bin[:, i], y_pred_bin[:, i]) for i in range(y_true_bin.shape[1])]
    mAP = np.mean(ap_scores)

    return accuracy, precision, recall, mAP

# Run experiments for each augmentation
for aug in ["augment1", "augment2", "augment3"]:
    print(f"Running experiment with {aug}")
    trained_model, loss_values, all_labels, all_preds, all_img_paths, all_ann_paths = run_experiment(loaders[aug], num_epochs, learning_rate, batch_size, optimizer_type)

    # Visualize loss
    plt.figure(figsize=(10, 5))
    plt.plot(range(1, num_epochs + 1), loss_values, marker='o')
    plt.title(f'Loss vs Epochs - {aug}')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.grid(True)
    plt.show()

    # Compute confusion matrix
    cm = confusion_matrix(all_labels, all_preds)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(class_dict.keys()))
    disp.plot(cmap=plt.cm.Blues)
    plt.title(f'Confusion Matrix - {aug}')
    plt.show()

    # Compute metrics
    accuracy, precision, recall, mAP = compute_metrics(all_labels, all_preds, class_dict)
    print(f"Augmentation: {aug}")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"Mean Average Precision (mAP): {mAP:.4f}")

"""Wyświetlanie zdjęc z porownaniem Real vs Predicted

"""

def get_bboxes(path_to_ann):
    tree = ET.parse(path_to_ann)
    root = tree.getroot()

    annotations = []

    for bndbox in root.iter('bndbox'):
        xmin = int(bndbox.find('xmin').text)
        ymin = int(bndbox.find('ymin').text)
        xmax = int(bndbox.find('xmax').text)
        ymax = int(bndbox.find('ymax').text)

        annotations.append([xmin, ymin, xmax, ymax])
    return annotations

img_dir = "dataset/images"
ann_dir = "dataset/annotations"

img_list = [os.path.join(img_dir, filename) for filename in os.listdir(img_dir)]
ann_list = [os.path.join(ann_dir, filename) for filename in os.listdir(ann_dir)]

img_list.sort()
ann_list.sort()

# Display images with bounding boxes and label based on specified ranges
print(" stop: 0,speedlimit: 1,trafficlight: 2,crosswalk: 3")

for i in range(35):
    img_path = img_list[i]
    ann_path = ann_list[i]

    img = np.asarray(Image.open(img_path))
    bboxes = get_bboxes(ann_path)

    plt.imshow(img)
    ax = plt.gca()

    for bbox in bboxes:
        x = bbox[0]
        y = bbox[1]
        width = bbox[2] - bbox[0]
        height = bbox[3] - bbox[1]
        rect = Rectangle((x, y), width, height, linewidth=1, edgecolor='r', facecolor='none')
        ax.add_patch(rect)

 # Labeling based on specified ranges

    if i+1 in range(28, 36):
        plt.title(f"Real:{all_labels[i]} , Predykcja: {all_preds[i]}")  # Information Signs
    elif 1 <= i+1 <= 3 or i+1 == 14 or i+1 == 25:
        plt.title(f"Real:{all_labels[i]}, Predykcja: {all_preds[i]}")  # Traffic Signals
    elif 4 <= i+1 <= 27:
        plt.title(f"Real:{all_labels[i]}, Predykcja: {all_preds[i]}")  # Speed Limits


    plt.axis('off')
    plt.show()

# Prepare bounding box data for scatter plot
bbox_positions = []  # List to store bounding box positions (centroid)
bbox_sizes = []  # List to store bounding box sizes (width, height)

for ann_path in all_ann_paths:
    bboxes = get_bboxes(ann_path)
    for bbox in bboxes:
        xmin, ymin, xmax, ymax = bbox
        x_center = (xmin + xmax) / 2
        y_center = (ymin + ymax) / 2
        bbox_positions.append((x_center, y_center))
        bbox_width = xmax - xmin
        bbox_height = ymax - ymin
        bbox_sizes.append((bbox_width, bbox_height))

# Unzip bounding box positions and sizes
x_positions, y_positions = zip(*bbox_positions)
bbox_widths, bbox_heights = zip(*bbox_sizes)

# Scatter plots for bounding box data
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.scatter(x_positions, y_positions, alpha=0.5, label='Objects')
plt.title('Bounding Box Positions')
plt.xlabel('X')
plt.ylabel('Y')

plt.subplot(1, 2, 2)
plt.scatter(bbox_widths, bbox_heights, alpha=0.5, label='Objects')
plt.title('Bounding Box Sizes')
plt.xlabel('Width')
plt.ylabel('Height')

plt.legend()
plt.tight_layout()
plt.show()

"""Przedstawienie predykcji dla danych testowych - 3 grupy danych"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
from torchvision import datasets, models, transforms
from PIL import Image
import xml.etree.ElementTree as ET
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, average_precision_score
import numpy as np
from sklearn.preprocessing import label_binarize

# Ścieżki do katalogów obrazów i adnotacji
img_dir = "dataset/images"
ann_dir = "dataset/annotations"

# Lista nazw plików obrazów i adnotacji
img_list = [os.path.join(img_dir, filename) for filename in os.listdir(img_dir)]
ann_list = [os.path.join(ann_dir, filename) for filename in os.listdir(ann_dir)]

# Słownik mapujący nazwy klas na indeksy klas
class_dict = {
    "stop": 0,
    "speedlimit": 1,
    "trafficlight": 2,
    "crosswalk": 3
}

# Tworzenie klasy Dataset
class CustomDataset(Dataset):
    def __init__(self, img_list, ann_list, class_dict, transform=None):
        self.img_list = img_list
        self.ann_list = ann_list
        self.class_dict = class_dict
        self.transform = transform

    def __len__(self):
        return len(self.img_list)

    def __getitem__(self, idx):
        img_path = self.img_list[idx]
        ann_path = self.ann_list[idx]
        image = Image.open(img_path).convert('RGB')

        tree = ET.parse(ann_path)
        root = tree.getroot()
        class_indices = []
        for name_tag in root.iter('name'):
            class_name = name_tag.text
            class_indices.append(self.class_dict.get(class_name))

        # Zmiana listy indeksów klas na pojedynczą etykietę klasy
        class_label = max(class_indices)

        if self.transform:
            image = self.transform(image)

        return image, class_label

# Definiowanie transformacji dla danych treningowych z różnymi augmentacjami
data_transforms = {
    "augment2": transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomHorizontalFlip(),  # Augmentacja 1: Losowe poziome odbicie
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ]),
    "augment1": transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  # Augmentacja 2: Losowa zmiana kolorów
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ]),
    "augment3": transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomRotation(20),  # Augmentacja 3: Losowy obrót
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ]),
    "test": transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
}

# Tworzenie datasetu i DataLoadera dla każdej augmentacji
datasets = {x: CustomDataset(img_list, ann_list, class_dict, transform=data_transforms[x]) for x in data_transforms}

# Funkcja do podziału danych na treningowe i testowe
def split_dataset(dataset, group_type):
    if group_type == 10:
        test_size = 10
    elif group_type == 50:
        test_size = 50
    elif group_type == 100:
        test_size = 100
    else:
        raise ValueError("Nieobsługiwana grupa testowa.")

    train_size = len(dataset) - test_size
    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])
    return train_dataset, test_dataset


# Definiowanie modelu ResNet-18
def get_resnet18(num_classes):
    model = models.resnet18(pretrained=True)
    num_ftrs = model.fc.in_features
    model.fc = nn.Linear(num_ftrs, num_classes)
    return model

# Funkcja do przeprowadzania eksperymentów z różnymi parametrami
def run_experiment(train_loader, test_loader, num_epochs, learning_rate, batch_size, optimizer_type):
    # Tworzenie modelu
    model = get_resnet18(num_classes=len(class_dict))
    model.train()


    # Wybór optymalizatora
    if optimizer_type == "Adam":
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    elif optimizer_type == "SGD":
        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)
    else:
        raise ValueError("Nieobslugiwany typ optymalizatora. Wspierane typy to: 'Adam' i 'SGD'.")

    criterion = nn.CrossEntropyLoss()

    # Przeprowadzenie treningu
    loss_values = []
    for epoch in range(num_epochs):
        running_loss = 0.0
        for i, data in enumerate(train_loader, 0):
            inputs, labels = data
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        epoch_loss = running_loss / len(train_loader)
        loss_values.append(epoch_loss)
        print(f"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}")
  #zapisanie modelu
    torch.save(model.state_dict(),"model_zapisany");

    # Ewaluacja modelu na danych testowych
    model.eval()
    all_labels = []
    all_preds = []
    with torch.no_grad():
        for data in test_loader:
            inputs, labels = data
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            all_labels.extend(labels.cpu().numpy())
            all_preds.extend(preds.cpu().numpy())

    return model, loss_values, all_labels, all_preds


# Parametry eksperymentu
num_epochs = 5  # Liczba epok
learning_rate = 0.001  # Współczynnik uczenia
batch_size = 32  # Rozmiar batcha
optimizer_type = "SGD"  # Wybór optymalizatora
group_type = [50]  # Grupy testowe: 10, 25, 100 elementów

# Funkcja do obliczania metryk
def compute_metrics(all_labels, all_preds, class_dict):
    # Konwersja list na tablice numpy
    all_labels = np.array(all_labels)
    all_preds = np.array(all_preds)

    # Dokładność
    accuracy = np.mean(all_labels == all_preds)

    # Precyzja i czułość
    precision = precision_score(all_labels, all_preds, average='macro')
    recall = recall_score(all_labels, all_preds, average='macro')

    # Średnia precyzja dla każdej klasy (mAP)
    # Binarizacja wyniku dla każdej klasy
    y_true_bin = label_binarize(all_labels, classes=list(class_dict.values()))
    y_pred_bin = label_binarize(all_preds, classes=list(class_dict.values()))

    # Obliczenie mAP dla każdej klasy i obliczenie średniej
    ap_scores = [average_precision_score(y_true_bin[:, i], y_pred_bin[:, i]) for i in range(y_true_bin.shape[1])]
    mAP = np.mean(ap_scores)

    return accuracy, precision, recall, mAP

# Przeprowadzenie eksperymentów dla każdej augmentacji
for aug in ["augment1", "augment2", "augment3"]:
    print(f"Running experiment with {aug}")

    for test_size in group_type:
        print(f"Group size: {test_size}")

        # Podział danych na treningowe i testowe
        train_dataset, test_dataset = split_dataset(datasets[aug], test_size)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

        # Przeprowadzenie eksperymentu
        trained_model, loss_values, all_labels, all_preds = run_experiment(train_loader, test_loader, num_epochs, learning_rate, batch_size, optimizer_type)

        # Wizualizacja strat
        plt.figure(figsize=(10, 5))
        plt.plot(range(1, num_epochs + 1), loss_values, marker='o')
        plt.title(f'Loss in epochs - {aug} (Group size: {test_size})')
        plt.xlabel('Epochs')
        plt.ylabel('Loss')
        plt.grid(True)
        plt.show()

        # Obliczanie metryk
        accuracy, precision, recall, mAP = compute_metrics(all_labels, all_preds, class_dict)
        print(f"Augmentation: {aug}, Group size: {test_size}")
        print(f"Accuracy: {accuracy:.4f}")
        print(f"Precision: {precision:.4f}")
        print(f"Recall: {recall:.4f}")
        print(f"Mean Average Precision (mAP): {mAP:.4f}")

model.load_state_dict(torch.load("model_zapisany"))
#test_dataset

#test dla 50
for i, data in enumerate(test_loader, 0):
            inputs, labels = data
            model.eval()
            outputs = model(inputs)
            print(outputs)

# model.eval()
# output = model([img,]);

print(f"Preds: {all_preds}")
print(f"Real: {all_labels}")
 # Accuracy
accuracy = np.mean(all_labels == all_preds)

    # Precision and Recall
precision = precision_score(all_labels, all_preds, average='macro')
recall = recall_score(all_labels, all_preds, average='macro')